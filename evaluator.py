import numpy as np
import logging
from typing import Dict, List, Tuple, Any, Optional
import nltk
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from nltk.tokenize import word_tokenize, sent_tokenize
from rouge_score import rouge_scorer
import matplotlib.pyplot as plt
import json
import os

# Download NLTK resources if needed
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SummaryEvaluator:
    """
    Evaluator for text summarization models, calculating metrics like ROUGE and BLEU.
    """
    
    def __init__(self):
        """Initialize the evaluator with required metrics."""
        # Initialize Rouge scorer
        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
        
        # Benchmarks from the problem statement
        self.benchmarks = {
            'PEGASUS': {'rouge1': 45.1, 'rouge2': 21.8, 'rougeL': 42.3, 'bleu': 36.2},
            'BART': {'rouge1': 43.5, 'rouge2': 19.4, 'rougeL': 40.6, 'bleu': 33.8},
            'Longformer': {'rouge1': 41.2, 'rouge2': 18.9, 'rougeL': 39.1, 'bleu': 32.4},
            'LED': {'rouge1': 40.5, 'rouge2': 17.8, 'rougeL': 38.6, 'bleu': 31.7},
            'GPT-4-Summarization': {'rouge1': 39.2, 'rouge2': 16.5, 'rougeL': 37.2, 'bleu': 30.8}
        }
        
        # Directory for storing evaluation results
        if not os.path.exists('evaluations'):
            os.makedirs('evaluations')
    
    def evaluate_summary(self, generated_summary: str, reference_summary: str) -> Dict[str, float]:
        """
        Evaluate a generated summary against a reference summary.
        
        Args:
            generated_summary: The summary generated by the model
            reference_summary: The ground truth summary
            
        Returns:
            Dictionary with evaluation metrics
        """
        metrics = {}
        
        # Preprocess summaries
        gen_summary = self._preprocess_text(generated_summary)
        ref_summary = self._preprocess_text(reference_summary)
        
        # Skip evaluation if either summary is empty
        if not gen_summary or not ref_summary:
            logger.warning("Empty summary detected, skipping evaluation")
            return {
                'rouge1': 0.0,
                'rouge2': 0.0, 
                'rougeL': 0.0,
                'bleu': 0.0
            }
        
        # Calculate ROUGE scores
        try:
            rouge_scores = self.rouge_scorer.score(ref_summary, gen_summary)
            metrics['rouge1'] = rouge_scores['rouge1'].fmeasure * 100
            metrics['rouge2'] = rouge_scores['rouge2'].fmeasure * 100
            metrics['rougeL'] = rouge_scores['rougeL'].fmeasure * 100
        except Exception as e:
            logger.error(f"Error calculating ROUGE scores: {str(e)}")
            metrics['rouge1'] = 0.0
            metrics['rouge2'] = 0.0
            metrics['rougeL'] = 0.0
        
        # Calculate BLEU score
        try:
            metrics['bleu'] = self._calculate_bleu(gen_summary, ref_summary)
        except Exception as e:
            logger.error(f"Error calculating BLEU score: {str(e)}")
            metrics['bleu'] = 0.0
        
        # Calculate readability metrics
        try:
            metrics.update(self._calculate_readability(gen_summary))
        except Exception as e:
            logger.error(f"Error calculating readability metrics: {str(e)}")
            metrics['readability'] = 0.0
        
        # Calculate summary statistics
        metrics['word_count'] = len(gen_summary.split())
        metrics['sentence_count'] = len(sent_tokenize(gen_summary))
        
        return metrics
    
    def _preprocess_text(self, text: str) -> str:
        """Preprocess text for evaluation."""
        if not text:
            return ""
        
        # Convert to lowercase
        text = text.lower()
        
        # Remove extra whitespace
        text = ' '.join(text.split())
        
        return text
    
    def _calculate_bleu(self, generated: str, reference: str) -> float:
        """Calculate BLEU score between generated and reference text."""
        # Tokenize into words
        gen_tokens = word_tokenize(generated)
        ref_tokens = word_tokenize(reference)
        
        # Calculate BLEU score with smoothing
        smoothing = SmoothingFunction().method1
        score = sentence_bleu([ref_tokens], gen_tokens, smoothing_function=smoothing)
        
        # Scale to 0-100 for consistency with ROUGE
        return score * 100
    
    def _calculate_readability(self, text: str) -> Dict[str, float]:
        """Calculate readability metrics."""
        readability_scores = {}
        
        # Calculate average word length
        words = text.split()
        if not words:
            return {'avg_word_length': 0, 'avg_sentence_length': 0}
        
        avg_word_length = sum(len(word) for word in words) / len(words)
        readability_scores['avg_word_length'] = avg_word_length
        
        # Calculate average sentence length
        sentences = sent_tokenize(text)
        if not sentences:
            readability_scores['avg_sentence_length'] = 0
            return readability_scores
        
        avg_sentence_length = len(words) / len(sentences)
        readability_scores['avg_sentence_length'] = avg_sentence_length
        
        return readability_scores
    
    def compare_with_benchmarks(self, metrics: Dict[str, float]) -> Dict[str, Any]:
        """
        Compare evaluation metrics with benchmark models.
        
        Args:
            metrics: Dictionary of evaluation metrics
            
        Returns:
            Dictionary with comparison results
        """
        comparison = {
            'our_model': {
                'name': 'Our Hybrid Model',
                'metrics': metrics
            },
            'benchmarks': self.benchmarks,
            'ranking': {}
        }
        
        # Add our model to the benchmarks for ranking
        all_models = {**self.benchmarks, 'Our Hybrid Model': metrics}
        
        # Rank models for each metric
        for metric in ['rouge1', 'rouge2', 'rougeL', 'bleu']:
            ranking = sorted(all_models.items(), key=lambda x: x[1].get(metric, 0), reverse=True)
            comparison['ranking'][metric] = [{'name': name, 'score': score.get(metric, 0)} for name, score in ranking]
        
        # Overall ranking (average of all metrics)
        def get_avg_score(model_scores):
            metrics_to_avg = ['rouge1', 'rouge2', 'rougeL', 'bleu']
            available_scores = [model_scores.get(m, 0) for m in metrics_to_avg]
            return sum(available_scores) / len(available_scores) if available_scores else 0
        
        overall_ranking = sorted(all_models.items(), key=lambda x: get_avg_score(x[1]), reverse=True)
        comparison['ranking']['overall'] = [{'name': name, 'score': get_avg_score(score)} for name, score in overall_ranking]
        
        # Position of our model in the overall ranking
        our_position = next((i+1 for i, item in enumerate(overall_ranking) if item[0] == 'Our Hybrid Model'), -1)
        comparison['our_ranking'] = our_position
        
        return comparison
    
    def visualize_comparison(self, comparison: Dict[str, Any], output_path: str = 'evaluations/comparison.json') -> None:
        """
        Generate visualizations for model comparison.
        
        Args:
            comparison: Comparison results from compare_with_benchmarks
            output_path: Path to save the JSON data
        """
        # Save comparison data
        with open(output_path, 'w') as f:
            json.dump(comparison, f)
        
        # Create visualization data
        models = list(self.benchmarks.keys()) + ['Our Hybrid Model']
        metrics = ['rouge1', 'rouge2', 'rougeL', 'bleu']
        
        # Prepare data for plotting
        data = []
        for model in models:
            if model == 'Our Hybrid Model':
                model_metrics = comparison['our_model']['metrics']
            else:
                model_metrics = self.benchmarks.get(model, {})
            
            model_data = [model_metrics.get(metric, 0) for metric in metrics]
            data.append(model_data)
        
        # Create colorful bar chart data
        x = np.arange(len(metrics))
        width = 0.15
        colors = ['#3366cc', '#dc3912', '#ff9900', '#109618', '#990099', '#0099c6']
        
        # Data for table visualization
        table_data = {
            'models': models,
            'metrics': metrics,
            'data': data
        }
        
        # Return visualization data
        return table_data
    
    def evaluate_batch(self, generated_summaries: List[str], reference_summaries: List[str]) -> Dict[str, Any]:
        """
        Evaluate a batch of generated summaries against reference summaries.
        
        Args:
            generated_summaries: List of summaries generated by the model
            reference_summaries: List of ground truth summaries
            
        Returns:
            Dictionary with aggregated evaluation metrics
        """
        # Ensure we have matching summaries
        n = min(len(generated_summaries), len(reference_summaries))
        
        all_metrics = []
        for i in range(n):
            metrics = self.evaluate_summary(generated_summaries[i], reference_summaries[i])
            all_metrics.append(metrics)
        
        # Aggregate metrics
        aggregated = {
            'individual': all_metrics,
            'average': self._aggregate_metrics(all_metrics),
            'count': n
        }
        
        # Compare with benchmarks
        comparison = self.compare_with_benchmarks(aggregated['average'])
        aggregated['comparison'] = comparison
        
        return aggregated
    
    def _aggregate_metrics(self, metrics_list: List[Dict[str, float]]) -> Dict[str, float]:
        """Aggregate metrics from multiple evaluations."""
        if not metrics_list:
            return {}
        
        # Initialize with keys from first item
        aggregated = {k: 0.0 for k in metrics_list[0].keys()}
        
        # Sum all values
        for metrics in metrics_list:
            for k, v in metrics.items():
                if k in aggregated:
                    aggregated[k] += v
        
        # Calculate averages
        n = len(metrics_list)
        for k in aggregated:
            aggregated[k] /= n
        
        return aggregated
